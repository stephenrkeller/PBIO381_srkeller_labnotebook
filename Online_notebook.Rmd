# Online Notebook    

### Author: Stephen Keller     
### PBIO/BIO381: Ecological Genomics, 2017
### PBIO/BIO381: Ecological Genomics, 2018

## Date started: (2017-01-18)   
## Date end:    

## Description   
This notebook will be a repository of all my online work throughout the semester. I will use it to log my notes, scripts, results, command-line output, and documents/assignments. Having it on github will allow both version control on this work, as well as foster reproducibility of the science in future semesters. 
​      

### Table of contents        
* [Page 1: 2017-01-17](#id-section1). Initial entry
* [Page 2: 2017-02-06](#id-section2). Intro to RNAseq
* [Page 3: 2017-02-08](#id-section3). RNASeq trimming, fastqc, and assembly
* [Page 4: 2017-02-13](#id-section4). RNA mapping
* [Page 5: 2017-02-22](#id-section5). RNASeq differential expression in DESeq2
* [Page 6: 2017-02-27](#id-section6) RNASeq cont…learning to build custom DE models in DESeq2
* [Page 7 2017-03-05:](#id-section7) Background work for calling SNPs in the mapped RNASeq reads. 
* [Page 8 2017-03-30:](#id-section8). Prep for selection scans
* [Page 9 2017-04-10:](#id-section9).Qiime 16S analysis
* [Page 10 2018-01-24:](#id-section10). Welcome to a new semester!
* [Page 11:](#id-section11).
* [Page 12:](#id-section12).
* [Page 13:](#id-section13).
* [Page 14:](#id-section14).
* [Page 15:](#id-section15).
* [Page 16:](#id-section16).
* [Page 17:](#id-section17).
* [Page 18:](#id-section18).
* [Page 19:](#id-section19).
* [Page 20:](#id-section20).
* [Page 21:](#id-section21).
* [Page 22:](#id-section22).
* [Page 23:](#id-section23).
* [Page 24:](#id-section24).
* [Page 25:](#id-section25).
* [Page 26:](#id-section26).
* [Page 27:](#id-section27).
* [Page 28:](#id-section28).
* [Page 29:](#id-section29).
* [Page 30:](#id-section30).
* [Page 31:](#id-section31).
* [Page 32:](#id-section32).
* [Page 33:](#id-section33).
* [Page 34:](#id-section34).
* [Page 35:](#id-section35).
* [Page 36:](#id-section36).
* [Page 37:](#id-section37).
* [Page 38:](#id-section38).
* [Page 39:](#id-section39).
* [Page 40:](#id-section40).
* [Page 41:](#id-section41).
* [Page 42:](#id-section42).
* [Page 43:](#id-section43).
* [Page 44:](#id-section44).
* [Page 45:](#id-section45).
* [Page 46:](#id-section46).
* [Page 47:](#id-section47).
* [Page 48:](#id-section48).
* [Page 49:](#id-section49).
* [Page 50:](#id-section50).
* [Page 51:](#id-section51).
* [Page 52:](#id-section52).
* [Page 53:](#id-section53).
* [Page 54:](#id-section54).
* [Page 55:](#id-section55).
* [Page 56:](#id-section56).
* [Page 57:](#id-section57).
* [Page 58:](#id-section58).
* [Page 59:](#id-section59).
* [Page 60:](#id-section60).

------
<div id='id-section1'/>
### Page 1: 2017-01-17.  Initial entry   


To embed an image, start with the "!", then [], then the URL in ()
```
![](URL)
```
![](https://cloud.githubusercontent.com/assets/12184909/22031778/c2232e86-dcaf-11e6-90c9-782206dcd824.png)

This image is a fastqc plot of Brittany's spruce GBS data

Here's an example of pasting output from the commandline:

```
stephen@Wright:.../Centaurea_GBS/fastq$ zcat srkeller_GBS_20160812_C1_keller_R1_001.fastq.gz | head
@HISEQ:140:160817_SNL128_0140_AC96CYACXX:8:1101:1425:1938 1:N:0:
NTTGTGACAGCTTCCTGAAAATCAAACAAAGAAATTAACAAACCGATCATGAGATCGGTAGACTTAATTCATGAGAGATTTACCTGAAAAAACGTGGCAGA
+
#0<<BBFFFFFFFFFFFFBFBFFFBBBBF0F<FBFIFFFFFBFFF7BFFIIIBBBFBFBFFFF<BFFFBFBFFBBBBBBBBBFBBBBBB<B<BBBBBBBBB
@HISEQ:140:160817_SNL128_0140_AC96CYACXX:8:1101:1398:1945 1:N:0:
NATGTCAGCGCTTCATGAAGCGCACCGGCAAGATGCCCAACATGATCCACGTCGGCACCTATTCGGCGGTGCTGAGATCGGAAGAGCGGTTCAGCAGGAAT
+
#0<FFFFFFFFFFIIIIIIIIIIIIIIIIIFFIFIIIIIFFIFIIIIFFFFFBFFFBFFBFFFFFFFFF0<BFF<7BBBFFFBBFBBFF77BBFBBFFFF<
@HISEQ:140:160817_SNL128_0140_AC96CYACXX:8:1101:1292:1959 1:N:0:
NCGGTAATACAGCGCCGACATGGAGCCCCTGCTGTAGCCATGGCTTGCTGGTTGCCCTCAATCCGGCCTAGTCAGCTGAGATCGGAAGAGCGGTTCAGCAG
stephen@Wright:.../Centaurea_GBS/fastq$ ll
total 12991974
drwxrwx--- 2 stephen klab           0 Jan  6 15:32 ./
drwxrwx--- 2 stephen klab           0 Jan 12 15:00 ../
drwxrwx--- 2 stephen klab           0 Jan  6 13:43 distribs/
-rw-rw---- 1 stephen klab        4096 Dec 22 12:36 ._.DS_Store
-rw-rw---- 1 stephen klab       10244 Jan 12 16:57 .DS_Store
drwxrwx--- 2 stephen klab           0 Jan  6 15:32 fastqc/
drwxrwx--- 2 stephen klab           0 Jan  5 17:33 old/
drwxrwx--- 2 stephen klab           0 Jan 12 16:57 parsed/
-rw-rw---- 1 stephen klab 13303765656 Jan  5 21:52 srkeller_GBS_20160812_C1_keller_R1_001.fastq.gz
drwxrwx--- 2 stephen klab           0 Jan  6 13:41 summaries/
stephen@Wright:.../Centaurea_GBS/fastq$ 

```

------
<div id='id-section2'/>
### Page 2: 2017-02-06. Intro to RNAseq

Practicing logging into server, looking at fastq files and understanding their format, assessing quality with fastqc, trimming, and re-evaluating qc.

Link to github [tutorial for today's RNASeq session.](https://adnguyen.github.io/2017_Ecological_Genomics/Tutorial/2017-02-06_RNAseq_tutorial.html) 

- Each student was assigned a single pair (R1 and R2) of fastq.gz files to work with. Melissa will do the remaining files not assigned to students.
- Students worked through editing their trimmomatic files (trim_example.sh) to specify the correct input and output paths, and running trimmomatic to clean up their reads. We stopped prior to running fastqc and setting up the Trinity runs. Will pick up there on Wednesday.
- ​

------
<div id='id-section3'/>
### Page 3: 2017-02-08. RNASeq trimming, fastqc, and assembly

We finished working through the fastq trimming and assessed quality using fastqc. Students scp'd their fastqc output to their local machines and we looked at the html file output together and interpreted.

This took us to the end of class essentially. We then discussed how to set up an assembly experiment using Trinity.

- Chose for Melissa and her postdoc (Melanie Lloyd) to do the assemblies outside of class time, and we would work with what they produced, going over in class the output stats from several different approaches and comparing & contrasting to determine which is best for our usage.

------
<div id='id-section4'/>
### Page 4: 2017-02-13. RNA Mapping

[New tutorial posted on Github: RNA Mapping](https://adnguyen.github.io/2017_Ecological_Genomics/Tutorial/2017-02-13_RNAseq_Mapping.html)

- We pick back up with a discussion of cleaned reads going into mapping assembly using Trinity. Melissa re-did the cleaning using different parameters: Q>20, base trim of 12 bp (=2 sets of random hexamers)
- Melissa has run assemblies and we will evaluate their quality based on output statistics.
- Melissa used individual seastar 15, which had all 5 days sampled, and went from healthy to sick within the period. Total number of reads across all samples was ~ 50M, which is high enough to give a good assembly.
- Output assembly from Trinity: 
  - /data/project_data/assembly/Trinity.fasta
- Steps to evaluate:

1. Predict ORFs using TransDecoder

   1. /data/project_data/assembly

2. Take output from TransDecoder to translate into aa's and BLASTp against uniprot_swissprot database

3. Now have final reference transcriptome that we will be working with from here on:

   /data/project_data/assembly/longest_orfs.cds

- We'll be using this reference assembly to start read mapping to reference genome with bwa:
  - bwaaln.sh script
  - Melissa did a nice explanation of her bash code here, basically the students just need to replace the first part of their R1 sample name, and the bash script generates the R2 sample name, and a shortened version of each sample name, and then pushes it to bwa. This script would also be useful to use in a for loop if mapping over many different .fq files.
  - Using default values in bwa for mapping (max diff = 0.04)




------
<div id='id-section5'/>
### Page 5:RNASeq differential expression in DESeq2

Running R markdown script for DESeq2 analysis



Have students scp the counts data onto their local hard drive:

```
scp /data/project_data/DGE/*
```

Note: Melissa made a new transcriptome assembly based on using 4 individuals

Link to the tutorial: [here!](https://adnguyen.github.io/2017_Ecological_Genomics/Tutorial/2017-2-22_RNAseqDGE.html)

Path to my R script on my local machine: 

```
~/github/PBIO381_srkeller_labnotebook/data/DGE_data/DESeq2_exploreSSW_trim.R
```



------
<div id='id-section6'/>
### Page 6: RNASeq cont…learning to build custom DE models in DESeq2

Melissa has re-run the assmebly and transcript counts and we are now going to work on new DE models in DESeq2

Path to R script and input data files:

`/Users/srkeller/github/PBIO381_srkeller_labnotebook/data/DGE_data/round2`

Key part of setting up a new DESeq design:

counts ~= covariate + testvariable —> first predictor is always the "control" covariate variable, the next ones are the ones of biological interest.

* complete dataset is now 13053 genes; of which 12954 have >100 counts (summed across all 77 samples)
* have a line in the R script to randomly sample the total gene set down to just 1200 in order to "practice" running different models
* Build different models:
  * interactions among multiple factors
  * build custom contrasts after analysis of all groups in a group-level design
  * time-series model




------
<div id='id-section7'/>
### Page 7: Background work for calling SNPs in the mapped RNASeq reads



Worked on this awhile today. Identified the following issues:

* Our samples are from the same genetic individuals but sampled from many different time points. The Read Group headers in the sam and bam alignment files (@RG) indicate that each library is being tracked as an independent sample. Thus, SNP calling will treat each library separately, which is not what we want.
* Options:
  * Regexp to change the @RG lines to include non-reduundant SM:"SampleID" info for each individual, then merge bam files into a single merged .bam prior to snp calling
  * choose a single "representative" bam file for calling snps
* Going to try the second approach for now to get things running, and then come back and do the first approach to generate a better SNP file for downstream analysis...
* ​

------
<div id='id-section8'/>
### Page 8: Prep for selection scans

After looking at the SSW SNP data for population structure (there isn't much), we want to test for divergent selection between Healthy and Sick status animals using 'BAYESCAN'. I'll use PGDSpider to convert from vcf > GESTE/BAYESCAN format.

Path: `~/PopGenmics/ssw/`

* Input files: 
  * SSW_bayescan_disease.pops 
    * Total N = 22 —> HH: N = 8; SS or HS: N = 14
  * SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf
    * N = 24; 5317 SNPs
  * vcf2bayescan.spid
    * settings file for conversion with PGDSpider
  * vcf2bayescan.sh
    * bash script that calls PGDSpider and runs conversion

Problem: there are 2 individuals (37 and 38) with 'MM' health status in the data. These don't have a reliable disease scoring, and so we are going to exclude them. Need to get them out of our vcf file using vcftools *—remove-indv* option

```
$ vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.gz --remove-indv 37 --remove-indv 38 --recode --out SSW_all_biallelic.MAF0.02.Miss0.8.recode.22inds
```

All good. Now set up bash script to run BAYESCAN with following settings:

```bash
#!/bin/bash
# PBIO/BIO 381: Spring 2017
# This script runs the program 'Bayescan' designed to identify markers under diversifying or balancing selection by analyzing SNP divergence among populations 

bayescan SSW_all_biallelic.MAF0.02.Miss0.8.recode.bayescan \
 -threads 10 \
# The number of threads you want to use for processing
 -n 5000 \
# The number of samples of the mcmc chain to output for calculating the posterior probabilities
 -thin 10 \
# The thinning interval between recorded samples for the mcmc chain. The total number of mcmc iterations is the number of samples * the thinning interval
 -nbp 20 \
# The number of pilot runs to make for tuning the initial model parameters
 -pilot 5000 \
# The length of the pilot runs
 -burn 50000 \
# The length of the burn-in period to discard before recording samples from the posterior
 -pr_odds 10 \
# The prior odds for the probability that a locus is neutral and not under any selection
 -od ./
# Sets the directory for putting results
```

* Things to think about:
  * Set up different population contrast: 
    * intertidal vs. subtidal
  * Try different pr_odds ratio: 
    * These are transcriptome data, so higher prob that 1 or more loci are under selection, or nearby to a SNP under selection
    * Previously, we've used 10,000 for pr_odds in GBS data (see also Lotterhos and Whitlock's papers on this issue); I think this would be too stringent on RNASeq SNPs where locii are closely linked per transcript

------
<div id='id-section9'/>
### Page 9: 16S analysis

Tutorial page: [16S day 1](https://adnguyen.github.io/2017_Ecological_Genomics/Tutorial/2017-04-10_16sAmipliconSeqData.html)

Using Qiime for this.

Have 100+ samples measured over 6 times points

Qiime will cluster all sequences into an OTU table, and then assign taxonomy using 'Greengenes'

Start with mapping file: 

```
/data/project_data/16s/map.txt
```

Barcode Sequence and LinkerPrimerSequence columns are empty (but need to be included for Qiime to be happy)

Phenotype: sick or healthy at the time of sampling

Pheno_num: How sick it was (0=healthy; 5=dead)

Final_phenotype: What it was at the end

Ran the following:

```
~/16s_analysis/validate_mapping_file.py -m /data/project_data/16s/map.txt -o validate_map -p -b
```

Look at html output file — OK to ignore missing data warnings

Datafiles:

```
/data/project_data/16s/data_files/*.fq.gz
```

cd to home dir 16s_analysis/ and execute:

```
multiple_join_paired_ends.py -i /data/project_data/16s/data_files -o ~/16s_analysis/joined --read1_indicator _R1 --read2_indicator _R2
```

run bash scripts to rename output files:

```
bash /data/project_data/16s/remove-underscore.sh
bash /data/project_data/16s/remove-R1.sh
```

Then demultiplex and clean reads (corrects a typo in the tutorial:

```
multiple_split_libraries_fastq.py -i ~/16s_analysis/joined -o ~/16s_analysis/filtered -m sampleid_by_file --include_input_dir_path --remove_filepath_in_name  --mapping_indicator ~/data/project_data/16s/map.txt
```

Now run extract seqs within the ./filtered directory

```
extract_seqs_by_sample_id.py -i seqs.fna -o test -s 04-5-05
```

Test looks fine (filled file) —> can now remove.

Set up long extract now (~ 4 days)

```
pick_open_reference_otus.py -i ~/16s_analysis/filtered/seqs.fna -o ~/16s_analysis/otus  --parallel --jobs_to_start 1
```

2 different ways to extract OTUs

Closed reference

* you give it a database, and it picks the best

Open reference

* uses similarity of sequences without referencing a db

CHeck a summary of the table to make sure all looks OK:

```
biom summarize-table -i /data/project_data/16s/otu_table/otu_table_mc2_w_tax_no_pynast_failures.biom

Num samples: 176
Num observations: 93033
Total count: 8362869
Table density (fraction of non-zero values): 0.028

Counts/sample summary:
 Min: 28412.0
 Max: 77866.0
 Median: 47051.500
 Mean: 47516.301
 Std. dev.: 7637.541
 Sample Metadata Categories: None provided
 Observation Metadata Categories: taxonomy

Counts/sample detail:
24-5-11: 28412.0
07-5-11: 31532.0
04-5-08: 32477.0
07-5-05: 32491.0
38-6-09: 33391.0
...
```

Number of observations = # OTUs that got identified



For next session: come with phyloseq installed in R and test the library





------
<div id='id-section10'/>
### Page 10: Welcome to a new semester!



```

```



------
<div id='id-section11'/>
### Page 11:
------
<div id='id-section12'/>
### Page 12:
------
<div id='id-section13'/>
### Page 13:
------
<div id='id-section14'/>
### Page 14:
------
<div id='id-section15'/>
### Page 15:
------
<div id='id-section16'/>
### Page 16:
------
<div id='id-section17'/>
### Page 17:
------
<div id='id-section18'/>
### Page 18:
------
<div id='id-section19'/>
### Page 19:
------
<div id='id-section20'/>
### Page 20:
------
<div id='id-section21'/>
### Page 21:
------
<div id='id-section22'/>
### Page 22:
------
<div id='id-section23'/>
### Page 23:
------
<div id='id-section24'/>
### Page 24:
------
<div id='id-section25'/>
### Page 25:
------
<div id='id-section26'/>
### Page 26:
------
<div id='id-section27'/>
### Page 27:
------
<div id='id-section28'/>
### Page 28:
------
<div id='id-section29'/>
### Page 29:
------
<div id='id-section30'/>
### Page 30:
------
<div id='id-section31'/>
### Page 31:
------
<div id='id-section32'/>
### Page 32:
------
<div id='id-section33'/>
### Page 33:
------
<div id='id-section34'/>
### Page 34:
------
<div id='id-section35'/>
### Page 35:
------
<div id='id-section36'/>
### Page 36:
------
<div id='id-section37'/>
### Page 37:
------
<div id='id-section38'/>
### Page 38:
------
<div id='id-section39'/>
### Page 39:
------
<div id='id-section40'/>
### Page 40:
------
<div id='id-section41'/>
### Page 41:
------
<div id='id-section42'/>
### Page 42:
------
<div id='id-section43'/>
### Page 43:
------
<div id='id-section44'/>
### Page 44:
------
<div id='id-section45'/>
### Page 45:
------
<div id='id-section46'/>
### Page 46:
------
<div id='id-section47'/>
### Page 47:
------
<div id='id-section48'/>
### Page 48:
------
<div id='id-section49'/>
### Page 49:
------
<div id='id-section50'/>
### Page 50:
------
<div id='id-section51'/>
### Page 51:
------
<div id='id-section52'/>
### Page 52:
------
<div id='id-section53'/>
### Page 53:
------
<div id='id-section54'/>
### Page 54:
------
<div id='id-section55'/>
### Page 55:
------
<div id='id-section56'/>
### Page 56:
------
<div id='id-section57'/>
### Page 57:
------
<div id='id-section58'/>
### Page 58:
------
<div id='id-section59'/>
### Page 59:
------
<div id='id-section60'/>
### Page 60:

------
